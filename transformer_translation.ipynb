{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIoXskULx4du",
        "outputId": "dd44533b-3233-4ec4-9720-dc0dc501c281"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opencc-python-reimplemented\n",
            "  Downloading opencc_python_reimplemented-0.1.7-py2.py3-none-any.whl (481 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.8/481.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: opencc-python-reimplemented\n",
            "Successfully installed opencc-python-reimplemented-0.1.7\n"
          ]
        }
      ],
      "source": [
        "!pip install opencc-python-reimplemented\n",
        "import time\n",
        "import pathlib\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import jieba\n",
        "from opencc import OpenCC  #pip install opencc-python-reimplemented"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 新段落"
      ],
      "metadata": {
        "id": "X84UJTQI7rPj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = pathlib.Path('/content/cmn.txt')\n",
        "path_to_file\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbAkiqNPx5EO",
        "outputId": "51692ff0-59f4-42fa-ede3-fc50ed8883ee"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('/content/cmn.txt')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#define a function to load the data, and store into a Chinese dataset, and an English dataset\n",
        "def load_data(path):\n",
        "    targ = [] #English dataset\n",
        "    inp = [] #Chinese dataset\n",
        "    text = path.read_text(encoding='utf-8') #load the data\n",
        "    lines = text.splitlines() #\n",
        "    pairs = [line.split('\\t')[:2] for line in lines]\n",
        "    for y, x in pairs: #read every pairs of dat\n",
        "        targ.append(y)\n",
        "        inp.append(x)\n",
        "    return targ, inp"
      ],
      "metadata": {
        "id": "YWFytQ7ByrxE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "targ, inp = load_data(path_to_file)\n",
        "print(len(inp))\n",
        "print(len(targ))\n",
        "print(inp[-10:])\n",
        "print(targ[-10:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yixwyNHDyubf",
        "outputId": "47e5c493-ce68-4a47-b3c8-6d4e9da868c5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "29909\n",
            "29909\n",
            "['菲律賓去年地震和海嘯造成了超過6000人的死亡。', '“又是汤姆的电话？” “嗯。最近他每天晚上都会打过来。当时就不该给他我的号码的。”', '我母亲的法语比我父亲的英语要好，所以他们通常用法语交流。', '汤姆不知如何翻译“计算机”一词，因为同他谈话的人从未见过一台。', '汤姆不喜欢使用”有色人种“这个术语，因为他认为，根据这种说法白种人没有颜色。', '你不想涂防晒霜是你的问题，但是晒伤了不要来抱怨。', '即使是现在，我偶尔还是想见到你。不是今天的你，而是我记忆中曾经的你。', '你很容易把母语说得通顺流畅，却很容易把非母语说得不自然。', '虽然我被公司解雇了，但是我还有点存款，所以目前不用担心生计问题。', '如果一個人在成人前沒有機會習得目標語言，他對該語言的認識達到母語者程度的機會是相當小的。']\n",
            "['Last year in the Philippines, earthquakes and tidal waves resulted in the deaths of more than 6,000 people.', '\"Is that Tom calling again?\" \"Yes. He calls every evening these days. I shouldn\\'t have given him my number.\"', 'My mother speaks French better than my father speaks English, so they usually speak to each other in French.', 'Tom didn\\'t know how to translate the word \"computer\" because the people he was talking to had never seen one.', 'Tom doesn\\'t like to use the term \"a person of color\" because he thinks it implies that white people have no color.', \"If you don't want to put on sunscreen, that's your problem. Just don't come complaining to me when you get a sunburn.\", \"Even now, I occasionally think I'd like to see you. Not the you that you are today, but the you I remember from the past.\", \"It's very easy to sound natural in your own native language, and very easy to sound unnatural in your non-native language.\", \"I got fired from the company, but since I have a little money saved up, for the time being, I won't have trouble with living expenses.\", \"If a person has not had a chance to acquire his target language by the time he's an adult, he's unlikely to be able to reach native speaker level in that language.\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Defines a funciton for converting traditional to simplified\n",
        "def proce_cn_sentence(cn_text):\n",
        "    text = cc.convert(cn_text)\n",
        "    text = jieba.lcut(text)\n",
        "    text = ' '.join(text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "9MJ_BCOpzKt6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cc = OpenCC('t2s')"
      ],
      "metadata": {
        "id": "PtZLrR0fyyaa"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#word segmentation of sentences.\n",
        "for i in range(len(inp)):\n",
        "    text = proce_cn_sentence(inp[i])\n",
        "    inp[i] = text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUSIQTBezNfQ",
        "outputId": "38307fd4-b184-4f76-d16f-e605f50be152"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "DEBUG:jieba:Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 1.621 seconds.\n",
            "DEBUG:jieba:Loading model cost 1.621 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "DEBUG:jieba:Prefix dict has been built successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inp[-10:] #check the data in inp list\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsO_6Uj5zSVK",
        "outputId": "37c4f7f5-f843-4af9-8cca-00c46f801931"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['菲律宾 去年 地震 和 海啸 造成 了 超过 6000 人 的 死亡 。',\n",
              " '“ 又 是 汤姆 的 电话 ？ ”   “ 嗯 。 最近 他 每天晚上 都 会 打 过来 。 当时 就 不该 给 他 我 的 号码 的 。 ”',\n",
              " '我 母亲 的 法语 比 我 父亲 的 英语 要 好 ， 所以 他们 通常 用 法语 交流 。',\n",
              " '汤姆 不知 如何 翻译 “ 计算机 ” 一词 ， 因为 同 他 谈话 的 人 从未见过 一台 。',\n",
              " '汤姆 不 喜欢 使用 ” 有色人种 “ 这个 术语 ， 因为 他 认为 ， 根据 这种 说法 白种人 没有 颜色 。',\n",
              " '你 不想 涂 防晒霜 是 你 的 问题 ， 但是 晒伤 了 不要 来 抱怨 。',\n",
              " '即使 是 现在 ， 我 偶尔 还是 想 见到 你 。 不是 今天 的 你 ， 而是 我 记忆 中 曾经 的 你 。',\n",
              " '你 很 容易 把 母语 说 得 通顺 流畅 ， 却 很 容易 把 非 母语 说 得 不 自然 。',\n",
              " '虽然 我 被 公司 解雇 了 ， 但是 我 还 有点 存款 ， 所以 目前 不用 担心 生计 问题 。',\n",
              " '如果 一个 人 在 成人 前 没有 机会 习得 目标语言 ， 他 对 该 语言 的 认识 达到 母语 者 程度 的 机会 是 相当 小 的 。']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#define the buffze size and bach size\n",
        "BUFFZE_SIZE = len(inp)\n",
        "print(BUFFZE_SIZE)\n",
        "BATCH_SIZE = 64"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzgTvhABzUpD",
        "outputId": "5e619263-5643-49cb-f253-0f5dcad9988b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "29909\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#load the data from list to dataset\n",
        "dataset = tf.data.Dataset.from_tensor_slices((inp, targ)).shuffle(BUFFZE_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "bqx6xS36zcWj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check the format of data in the dataset\n",
        "for example_input_batch, example_target_batch in dataset.take(1):\n",
        "    print(example_input_batch[:3])\n",
        "    print()\n",
        "    print(example_target_batch[:3])\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gw8oQhWHzcxy",
        "outputId": "a155bfb7-6474-4624-8fb8-fee9430bfec1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'\\xe7\\x9c\\x8b\\xe6\\x9d\\xa5 \\xe6\\x88\\x91 \\xe6\\x98\\x8e\\xe5\\xa4\\xa9 \\xe4\\xbc\\x9a \\xe6\\x9d\\xa5 \\xe3\\x80\\x82'\n",
            " b'\\xe5\\xa5\\xb9 \\xe6\\x98\\x8e\\xe5\\xb9\\xb4 \\xe5\\xb0\\x86\\xe8\\xa6\\x81 \\xe7\\xbb\\x93\\xe5\\xa9\\x9a \\xe3\\x80\\x82'\n",
            " b'\\xe8\\xbf\\x99\\xe6\\x9d\\xa1 \\xe8\\xb7\\xaf\\xe5\\xa4\\xaa\\xe7\\xaa\\x84 \\xe6\\xb1\\xbd\\xe8\\xbd\\xa6 \\xe6\\x97\\xa0\\xe6\\xb3\\x95 \\xe9\\x80\\x9a\\xe8\\xa1\\x8c \\xe3\\x80\\x82'], shape=(3,), dtype=string)\n",
            "\n",
            "tf.Tensor(\n",
            "[b\"I'll be here tomorrow, I think.\"\n",
            " b\"She'll be getting married next year.\"\n",
            " b'The road is too narrow for cars.'], shape=(3,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#add the start and end in every Chinese sentence\n",
        "def tf_lower_and_split_punct_cn(text):\n",
        "    #change the capital character in to lower\n",
        "    text = tf.strings.lower(text)\n",
        "    text = tf.strings.strip(text)\n",
        "    text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
        "    return text\n",
        "\n"
      ],
      "metadata": {
        "id": "sjV9CiFnzeoo"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_text = tf.constant('去年 在 菲律宾 ， 地震 和 海啸 造成 了 超过 6000 人 的 死亡 。')\n",
        "print(example_text.numpy().decode())\n",
        "print(tf_lower_and_split_punct_cn(example_text).numpy().decode())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIAJd6gpzibn",
        "outputId": "b7e38045-cfd1-4ea8-998c-30dcf73302a8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "去年 在 菲律宾 ， 地震 和 海啸 造成 了 超过 6000 人 的 死亡 。\n",
            "[START] 去年 在 菲律宾 ， 地震 和 海啸 造成 了 超过 6000 人 的 死亡 。 [END]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_vocab_size = 15000 #define the size of the Chinese dictionary\n"
      ],
      "metadata": {
        "id": "igi8edR6zj49"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tf.keras.layers.TextVectorization can change sentences into number\n",
        "\n",
        "\n",
        "input_text_processor = tf.keras.layers.TextVectorization(\n",
        "    standardize=tf_lower_and_split_punct_cn,\n",
        "    max_tokens=max_vocab_size)"
      ],
      "metadata": {
        "id": "imYKFxGCz2R0"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#use the method above\n",
        "input_text_processor.adapt(inp)\n",
        "\n",
        "#get the table of vocabulary\n",
        "input_text_processor.get_vocabulary()[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLtdPPPjz36X",
        "outputId": "f2bf0d68-6187-4280-a0ec-b3aa4c23aab4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '[UNK]', '[START]', '[END]', '。', '我', '的', '了', '你', '？']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_tokens = input_text_processor(example_input_batch)\n",
        "example_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6iVuCgGqz8RG",
        "outputId": "8ab1c5b7-a6ce-457f-b6d2-1ee2ef2e14e2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(64, 17), dtype=int64, numpy=\n",
              "array([[   2,  390,    5, ...,    0,    0,    0],\n",
              "       [   2,   15,  868, ...,    0,    0,    0],\n",
              "       [   2,  424, 7328, ...,    0,    0,    0],\n",
              "       ...,\n",
              "       [   2,   10,   30, ...,    0,    0,    0],\n",
              "       [   2,  858,  237, ...,    0,    0,    0],\n",
              "       [   2,    5,   18, ...,    0,    0,    0]])>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#the method to process the English sentences\n",
        "def tf_lower_and_split_punct_en(text):\n",
        "    text = tf.strings.lower(text)\n",
        "    text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n",
        "    text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n",
        "    text = tf.strings.strip(text)\n",
        "\n",
        "    text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
        "    return text"
      ],
      "metadata": {
        "id": "1mHxvIEF1uQx"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_text_processor = tf.keras.layers.TextVectorization(\n",
        "    standardize=tf_lower_and_split_punct_en,\n",
        "    max_tokens=max_vocab_size)"
      ],
      "metadata": {
        "id": "5xm3E5a91xbg"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_text_processor.adapt(targ)\n",
        "output_text_processor.get_vocabulary()[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-3xXJON1z_9",
        "outputId": "b9e2af5e-1e9b-44b2-fb32-de3683906d97"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '[UNK]', '[START]', '[END]', '.', 'the', 'i', 'to', 'you', '?']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_pairs(cn, en):\n",
        "    cn = input_text_processor(cn)\n",
        "    en = output_text_processor(en)\n",
        "    return cn, en"
      ],
      "metadata": {
        "id": "HWjprCxy11iU"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_batches(ds):\n",
        "    return (\n",
        "        ds\n",
        "        .map(tokenize_pairs, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    )"
      ],
      "metadata": {
        "id": "4AfblIdO1_-G"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_batches = make_batches(dataset)\n",
        "train_batches"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RG9zSkIy2Bo_",
        "outputId": "c3861154-7ef4-47ce-8128-4bf953bc3bb7"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_ParallelMapDataset element_spec=(TensorSpec(shape=(None, None), dtype=tf.int64, name=None), TensorSpec(shape=(None, None), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for (batch, (inp, tar)) in enumerate(train_batches.take(3)):\n",
        "    print(batch)\n",
        "    print(inp)\n",
        "    print(tar)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDwbnVvO2DQX",
        "outputId": "ea7718dd-a97d-4416-8408-4047859489fd"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "tf.Tensor(\n",
            "[[    2  4709     7     4     3     0     0     0     0     0     0     0\n",
            "      0]\n",
            " [    2    12   262    32   113     4     3     0     0     0     0     0\n",
            "      0]\n",
            " [    2   591  1035  1364   935   963     4     3     0     0     0     0\n",
            "      0]\n",
            " [    2    17    13   469  1335   358  7506     6     4     3     0     0\n",
            "      0]\n",
            " [    2     5  8587  1652     4     3     0     0     0     0     0     0\n",
            "      0]\n",
            " [    2    15    34     5  3181     7   488     4     3     0     0     0\n",
            "      0]\n",
            " [    2    10    13     5     6    31   854     4     3     0     0     0\n",
            "      0]\n",
            " [    2    10    20  6806     6   363 10706     4     3     0     0     0\n",
            "      0]\n",
            " [    2    17  3244     7    31   109     4     3     0     0     0     0\n",
            "      0]\n",
            " [    2    62    27   144   577     4     3     0     0     0     0     0\n",
            "      0]\n",
            " [    2     8    55   253    10  1459     4     3     0     0     0     0\n",
            "      0]\n",
            " [    2     8    26    15    78   154   367    14     9     3     0     0\n",
            "      0]\n",
            " [    2    15    23     7  2372   238   363     4     3     0     0     0\n",
            "      0]\n",
            " [    2     5    18    12  1121   302     4     3     0     0     0     0\n",
            "      0]\n",
            " [    2    53    12   173    21    14     9     3     0     0     0     0\n",
            "      0]\n",
            " [    2   593    20    95   507     7   141     9     3     0     0     0\n",
            "      0]\n",
            " [    2    11  4119     7    10     6  5518     4     3     0     0     0\n",
            "      0]\n",
            " [    2     8    50  5869    10  3496     6   798   511     9     3     0\n",
            "      0]\n",
            " [    2     5    18    26    10    21    58    35    47     4     3     0\n",
            "      0]\n",
            " [    2    85 12215     4     3     0     0     0     0     0     0     0\n",
            "      0]\n",
            " [    2     8    35    23  2836    14     9     3     0     0     0     0\n",
            "      0]\n",
            " [    2    11  5267   115     4     3     0     0     0     0     0     0\n",
            "      0]\n",
            " [    2    11    12  2991   361   160    64   125     7 11315     4     3\n",
            "      0]\n",
            " [    2     5   255     8  2738     4     3     0     0     0     0     0\n",
            "      0]\n",
            " [    2     8    66    26    22    13    18    82     6     4     3     0\n",
            "      0]\n",
            " [    2    11  1841     5     6  1462    24     7     4     3     0     0\n",
            "      0]\n",
            " [    2    15    12    10     6  2012    46  2435     7   138     4     3\n",
            "      0]\n",
            " [    2  1322    20  6542  1007    57     7     4     3     0     0     0\n",
            "      0]\n",
            " [    2     5   385     7   178    11     4     3     0     0     0     0\n",
            "      0]\n",
            " [    2  2354     4     3     0     0     0     0     0     0     0     0\n",
            "      0]\n",
            " [    2     8    90    10    13    86  2041   228   616 10362    14     9\n",
            "      3]\n",
            " [    2    29  9032    34    17   718  9033     4     3     0     0     0\n",
            "      0]\n",
            " [    2    56    49  2642     4    15   221  2251     4     3     0     0\n",
            "      0]\n",
            " [    2     5    13  3916     4     3     0     0     0     0     0     0\n",
            "      0]\n",
            " [    2   684   252    35    19    38     4     3     0     0     0     0\n",
            "      0]\n",
            " [    2     8   173   234     7  1514    34     5     4     3     0     0\n",
            "      0]\n",
            " [    2     5  5758    16    28   722  1751     4     3     0     0     0\n",
            "      0]\n",
            " [    2   869   112    21     5    25    75  2726   534     6     4     3\n",
            "      0]\n",
            " [    2     8   597    30   316    31    16  1171   205   611     9     3\n",
            "      0]\n",
            " [    2    36  6076     7    16   300   811   231     7   108     4     3\n",
            "      0]\n",
            " [    2    12   457  2464   108    12   129     5     4     3     0     0\n",
            "      0]\n",
            " [    2    11  1198     7    14     9     3     0     0     0     0     0\n",
            "      0]\n",
            " [    2     5     6   374   657     7     4     3     0     0     0     0\n",
            "      0]\n",
            " [    2  7212    11     6     4     3     0     0     0     0     0     0\n",
            "      0]\n",
            " [    2     8    35   785     6     4     3     0     0     0     0     0\n",
            "      0]\n",
            " [    2    39   219     7    16     5  9574     4     3     0     0     0\n",
            "      0]\n",
            " [    2    22   132    13    73     6     9     3     0     0     0     0\n",
            "      0]\n",
            " [    2   637   420 10633     6   243 11038     4     3     0     0     0\n",
            "      0]\n",
            " [    2     8   146   241     4     3     0     0     0     0     0     0\n",
            "      0]\n",
            " [    2     5   594     8    35   265     4     3     0     0     0     0\n",
            "      0]\n",
            " [    2    22    19    38     4     3     0     0     0     0     0     0\n",
            "      0]\n",
            " [    2   164    35   250    14     9     3     0     0     0     0     0\n",
            "      0]\n",
            " [    2     5   169    25    23   130     4     3     0     0     0     0\n",
            "      0]\n",
            " [    2     5    23   159  2673     4     3     0     0     0     0     0\n",
            "      0]\n",
            " [    2    15  9403     7     4     3     0     0     0     0     0     0\n",
            "      0]\n",
            " [    2    87  5195     6     4     3     0     0     0     0     0     0\n",
            "      0]\n",
            " [    2  2180  4503   146    12  2364     9     3     0     0     0     0\n",
            "      0]\n",
            " [    2    15  2035     6  1604     7  1670     4     3     0     0     0\n",
            "      0]\n",
            " [    2     5   206  1719   165  1639  5134     4     3     0     0     0\n",
            "      0]\n",
            " [    2  5315  8767    16 11704     5 11661     4     3     0     0     0\n",
            "      0]\n",
            " [    2     8  1809  3387   280    70     3     0     0     0     0     0\n",
            "      0]\n",
            " [    2    11    32     7    10    21    80    48   200     4     3     0\n",
            "      0]\n",
            " [    2     8    97   381   104     4     3     0     0     0     0     0\n",
            "      0]\n",
            " [    2     5   377    34     7     5   664   554     4     3     0     0\n",
            "      0]], shape=(64, 13), dtype=int64)\n",
            "tf.Tensor(\n",
            "[[   2    5  949   48  632    4    3    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   2  127   11  954   14  640    4    3    0    0    0    0    0    0\n",
            "     0]\n",
            " [   2    5 1138   51  103  144  305    4    3    0    0    0    0    0\n",
            "     0]\n",
            " [   2   33   65  721    7  783    4    3    0    0    0    0    0    0\n",
            "     0]\n",
            " [   2   32 1665    7 6635    4    3    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   2   25  114  411   26   21    4    3    0    0    0    0    0    0\n",
            "     0]\n",
            " [   2   13   11   82   15   16 1262    4    3    0    0    0    0    0\n",
            "     0]\n",
            " [   2   13   48   10 2243 7058   26  389    4    3    0    0    0    0\n",
            "     0]\n",
            " [   2   33 2825   10  343    4    3    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   2   47   38   17 1320    4    3    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   2    8   40  258   52   26  106    4    3    0    0    0    0    0\n",
            "     0]\n",
            " [   2   19    8   43   89  231   95  436    9    3    0    0    0    0\n",
            "     0]\n",
            " [   2   25  153    7 1608    7  307  389    4    3    0    0    0    0\n",
            "     0]\n",
            " [   2    6   28  346   14   10  292    4    3    0    0    0    0    0\n",
            "     0]\n",
            " [   2   27    8  296   26  130    9    3    0    0    0    0    0    0\n",
            "     0]\n",
            " [   2   30   80  130  550  807    9    3    0    0    0    0    0    0\n",
            "     0]\n",
            " [   2   12   81 1395   15   34  148  316    4    3    0    0    0    0\n",
            "     0]\n",
            " [   2   40    8  166   59   39    5  188   13 4413    9    3    0    0\n",
            "     0]\n",
            " [   2    6   28   43   75   13   51   78    4    3    0    0    0    0\n",
            "     0]\n",
            " [   2   28 5304    4    3    0    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   2   51    8   41    7 1614    9    3    0    0    0    0    0    0\n",
            "     0]\n",
            " [   2   12  262   10  142   15  123    4    3    0    0    0    0    0\n",
            "     0]\n",
            " [   2   12  508   14   17  480  125   26   10  178  175  250  162    4\n",
            "     3]\n",
            " [   2    6 2436    8    7  470  628    4    3    0    0    0    0    0\n",
            "     0]\n",
            " [   2    8   83   43   17  150  876    4    3    0    0    0    0    0\n",
            "     0]\n",
            " [   2   12   54   22    5  299  188    6   54    4    3    0    0    0\n",
            "     0]\n",
            " [   2   25 1603   52   31    5 2759    4    3    0    0    0    0    0\n",
            "     0]\n",
            " [   2 7060  791  320  123 1790    4    3    0    0    0    0    0    0\n",
            "     0]\n",
            " [   2    6  672    7  258   12    4    3    0    0    0    0    0    0\n",
            "     0]\n",
            " [   2  282    5  321    4    3    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   2   19    8   60   13   54    5  217   31   34  407    9    3    0\n",
            "     0]\n",
            " [   2   18 6583 1999   96   39  207   44 1631    4    3    0    0    0\n",
            "     0]\n",
            " [   2   62  105  121  362    4  231  173 1490    7   37  362    4    3\n",
            "     0]\n",
            " [   2    6  120   67  173  343    4    3    0    0    0    0    0    0\n",
            "     0]\n",
            " [   2   22   51   37  805   18  463    4    3    0    0    0    0    0\n",
            "     0]\n",
            " [   2    8  179   21    5  256  470    4    3    0    0    0    0    0\n",
            "     0]\n",
            " [   2    6   24  339 1382   44  221    7  268  130 1643    4    3    0\n",
            "     0]\n",
            " [   2   63  358   30  917   20    6  212  470   16  340    4    3    0\n",
            "     0]\n",
            " [   2  277   19    8  777   20  815  187  765    9    3    0    0    0\n",
            "     0]\n",
            " [   2   61  114   59   44  539  197  190    4    3    0    0    0    0\n",
            "     0]\n",
            " [   2    6   23   10  274  396   26   21   14  599   15    5  580    4\n",
            "     3]\n",
            " [   2   11   12  675    9    3    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   2   16  552  500   10  228    4    3    0    0    0    0    0    0\n",
            "     0]\n",
            " [   2   47  110  286  264   26   12    4    3    0    0    0    0    0\n",
            "     0]\n",
            " [   2  302   71  191    4    3    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   2   47   77    7  272   20   26  247    4    3    0    0    0    0\n",
            "     0]\n",
            " [   2  107  154    5  104 1090    7    9    3    0    0    0    0    0\n",
            "     0]\n",
            " [   2    5  913 3251   39    5 6070  592    4    3    0    0    0    0\n",
            "     0]\n",
            " [   2    8   27  129  183    4    3    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   2   32 2388   17  302  794    4    3    0    0    0    0    0    0\n",
            "     0]\n",
            " [   2   18   11   50   64    4    3    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   2   40  327  315    9    3    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   2    6   41    7   98  143  115    4    3    0    0    0    0    0\n",
            "     0]\n",
            " [   2   32   77    7    5 3002    4    3    0    0    0    0    0    0\n",
            "     0]\n",
            " [   2   25   24 5348    4    3    0    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   2   79   85    7   22    4    3    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   2   89   27    8  138  129 1915    9    3    0    0    0    0    0\n",
            "     0]\n",
            " [   2  231  247    7  794    4    3    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   2   87   88  239  747   10  525  419    4    3    0    0    0    0\n",
            "     0]\n",
            " [   2    5 5650   15    5 6866 5773   21    7  346    4    3    0    0\n",
            "     0]\n",
            " [   2   45 1266    8   27  124    3    0    0    0    0    0    0    0\n",
            "     0]\n",
            " [   2   12  136   13   73   85  140    4    3    0    0    0    0    0\n",
            "     0]\n",
            " [   2  302   23    7  486   35  318    4    3    0    0    0    0    0\n",
            "     0]\n",
            " [   2   16  502  179   21   10  649    4    3    0    0    0    0    0\n",
            "     0]], shape=(64, 15), dtype=int64)\n",
            "1\n",
            "tf.Tensor(\n",
            "[[    2    10    33   156     6  6002  2311     7  1321     4     3     0\n",
            "      0     0     0]\n",
            " [    2    17    18  1143     4     3     0     0     0     0     0     0\n",
            "      0     0     0]\n",
            " [    2    22    49    31    19   298     6   637     4     3     0     0\n",
            "      0     0     0]\n",
            " [    2     8    24    21     9     3     0     0     0     0     0     0\n",
            "      0     0     0]\n",
            " [    2    11    12  3445    61  1999     7     4     3     0     0     0\n",
            "      0     0     0]\n",
            " [    2     8   569    78    39   154    24     9     3     0     0     0\n",
            "      0     0     0]\n",
            " [    2     5   135   318   609   509   680     6   155     4     3     0\n",
            "      0     0     0]\n",
            " [    2     5   103   570   444     4     3     0     0     0     0     0\n",
            "      0     0     0]\n",
            " [    2     5     6   470   767     7     4     3     0     0     0     0\n",
            "      0     0     0]\n",
            " [    2   349  1373    88  1152   192    43   104   173    61   460  5253\n",
            "      7     4     3]\n",
            " [    2     8    55    33     8     6  1162   581    59     4     3     0\n",
            "      0     0     0]\n",
            " [    2     5    18   379  6047   259     4     3     0     0     0     0\n",
            "      0     0     0]\n",
            " [    2    17   556  1745     4     3     0     0     0     0     0     0\n",
            "      0     0     0]\n",
            " [    2    37   778    47  3327     4     3     0     0     0     0     0\n",
            "      0     0     0]\n",
            " [    2   144   179    40    17   104  1520    98     4     3     0     0\n",
            "      0     0     0]\n",
            " [    2   149    94  1061   361     4     3     0     0     0     0     0\n",
            "      0     0     0]\n",
            " [    2   220   204    63    25   403  5433     4     3     0     0     0\n",
            "      0     0     0]\n",
            " [    2    10    19   233   657     4     3     0     0     0     0     0\n",
            "      0     0     0]\n",
            " [    2    15   429    94  6382   120    58    25   350     4     3     0\n",
            "      0     0     0]\n",
            " [    2    10   221   301     4     3     0     0     0     0     0     0\n",
            "      0     0     0]\n",
            " [    2   180  1457   203   185     4     3     0     0     0     0     0\n",
            "      0     0     0]\n",
            " [    2    45    67     5    11    19    38     4     3     0     0     0\n",
            "      0     0     0]\n",
            " [    2    17   232   343   112     6    72    16   275    19   547     4\n",
            "      3     0     0]\n",
            " [    2   202     5     6   295    61   460     7     4     3     0     0\n",
            "      0     0     0]\n",
            " [    2   991  1123    20   847  2106     4     3     0     0     0     0\n",
            "      0     0     0]\n",
            " [    2     5  2757     7    31   251    16   522  1132  2063     7     4\n",
            "      3     0     0]\n",
            " [    2    10  1236    57    31 10721  1199     7     4     3     0     0\n",
            "      0     0     0]\n",
            " [    2    44     5   304    22    43    49    52    76     4     3     0\n",
            "      0     0     0]\n",
            " [    2    60    88    75   112     4     3     0     0     0     0     0\n",
            "      0     0     0]\n",
            " [    2    22    13     8     6   140     4     5     6   141     9     3\n",
            "      0     0     0]\n",
            " [    2  1052  2735     4     3     0     0     0     0     0     0     0\n",
            "      0     0     0]\n",
            " [    2    87   250     4     3     0     0     0     0     0     0     0\n",
            "      0     0     0]\n",
            " [    2     8    63    12    93    54     9     3     0     0     0     0\n",
            "      0     0     0]\n",
            " [    2     8  1773    65    17   153  1016    14     9     3     0     0\n",
            "      0     0     0]\n",
            " [    2    36   146  3672     4     3     0     0     0     0     0     0\n",
            "      0     0     0]\n",
            " [    2    10   571    54   286  1695     4     3     0     0     0     0\n",
            "      0     0     0]\n",
            " [    2     8    55   128     5   874    22   565    14    41     3     0\n",
            "      0     0     0]\n",
            " [    2     5   644   192    77    91     7     4     3     0     0     0\n",
            "      0     0     0]\n",
            " [    2    10   270     7   450   259     4     3     0     0     0     0\n",
            "      0     0     0]\n",
            " [    2     5  1179  3556     6    58  3254     7    10  4121     6   331\n",
            "      4     3     0]\n",
            " [    2     5    74    12   392     4     3     0     0     0     0     0\n",
            "      0     0     0]\n",
            " [    2     8    74    66   836     4     3     0     0     0     0     0\n",
            "      0     0     0]\n",
            " [    2  3683  6560  4982     6  1379     4     3     0     0     0     0\n",
            "      0     0     0]\n",
            " [    2     5   110     5     6   619  1806   123    64    47     4     3\n",
            "      0     0     0]\n",
            " [    2    60    37    21    80   122    24     4     3     0     0     0\n",
            "      0     0     0]\n",
            " [    2  1332   335  7403     8     6  2294     4     3     0     0     0\n",
            "      0     0     0]\n",
            " [    2    10    94     5   185  1407     4     3     0     0     0     0\n",
            "      0     0     0]\n",
            " [    2    17     6   439    39  1697    51     7     4     3     0     0\n",
            "      0     0     0]\n",
            " [    2    15   983     5     6   410     4     3     0     0     0     0\n",
            "      0     0     0]\n",
            " [    2  5155     4     3     0     0     0     0     0     0     0     0\n",
            "      0     0     0]\n",
            " [    2   188   272     6     4     3     0     0     0     0     0     0\n",
            "      0     0     0]\n",
            " [    2    10  2552   628     4     3     0     0     0     0     0     0\n",
            "      0     0     0]\n",
            " [    2    36  1259     7    29  7727     6   257     4     3     0     0\n",
            "      0     0     0]\n",
            " [    2     5    26     8   122    31  6414     4     3     0     0     0\n",
            "      0     0     0]\n",
            " [    2     5   127    48    37   224     5   894     4     3     0     0\n",
            "      0     0     0]\n",
            " [    2     5   110   450   259     4     3     0     0     0     0     0\n",
            "      0     0     0]\n",
            " [    2     5    19  1206    10     6   405     4     3     0     0     0\n",
            "      0     0     0]\n",
            " [    2    11    33   764     5     6    83   923     5     7     4     3\n",
            "      0     0     0]\n",
            " [    2     5    44    15  1512     4     3     0     0     0     0     0\n",
            "      0     0     0]\n",
            " [    2    11    42     5   635   236     4     3     0     0     0     0\n",
            "      0     0     0]\n",
            " [    2    17 10160    71    16    13  1511  9026  3540     4     3     0\n",
            "      0     0     0]\n",
            " [    2    45     8    34     5    71   138    22   867   821    38    14\n",
            "      9     3     0]\n",
            " [    2     5    96    82    13  4024   141     9  4024    75  2481     4\n",
            "      3     0     0]\n",
            " [    2    11    13   741     4     3     0     0     0     0     0     0\n",
            "      0     0     0]], shape=(64, 15), dtype=int64)\n",
            "tf.Tensor(\n",
            "[[   2   13 1069 ...    0    0    0]\n",
            " [   2   65   38 ...    0    0    0]\n",
            " [   2   22  303 ...    0    0    0]\n",
            " ...\n",
            " [   2   76    8 ...    0    0    0]\n",
            " [   2   45  111 ...    0    0    0]\n",
            " [   2   12   11 ...    0    0    0]], shape=(64, 19), dtype=int64)\n",
            "2\n",
            "tf.Tensor(\n",
            "[[   2  297 2678 ...    0    0    0]\n",
            " [   2  193 2123 ...    0    0    0]\n",
            " [   2    8   50 ...    0    0    0]\n",
            " ...\n",
            " [   2   22   13 ...    0    0    0]\n",
            " [   2   10    6 ...    0    0    0]\n",
            " [   2    5   86 ...    0    0    0]], shape=(64, 16), dtype=int64)\n",
            "tf.Tensor(\n",
            "[[   2  207 4952 ...    0    0    0]\n",
            " [   2   28  448 ...    0    0    0]\n",
            " [   2   40    8 ...    0    0    0]\n",
            " ...\n",
            " [   2   18   11 ...    0    0    0]\n",
            " [   2   34  100 ...    0    0    0]\n",
            " [   2    6  446 ...    0    0    0]], shape=(64, 18), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PE represent the information of the position\n",
        "# pos represent the position of every word in the sentenece\n",
        "# embedding size is 300\n",
        "# i=0 时 2i=0 represent the first embedding position is even\n",
        "# when 2i is even we use sin, 2i+1 is odd we use cos\n",
        "\n",
        "\n",
        "# the calculation formula\n",
        "def get_angles(pos, i, d_model):\n",
        "    angle_rates = 1/np.power(10000, (2 * (i//2)) / np.float32(d_model)) #i//2取整\n",
        "    return pos * angle_rates\n",
        "\n",
        "\n",
        "#position represent the length of the sentence, d_model represent the dimension of embedding\n",
        "def positional_encoding(position, d_model):\n",
        "\n",
        "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                            np.arange(d_model)[np.newaxis, :],\n",
        "                            d_model)\n",
        "\n",
        "    # apply sin to even indices in the array; 2i\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "    # apply cos to odd indices in the array; 2i+1\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = angle_rads[np.newaxis, ...] #(1, position, d_model)\n",
        "\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32) #change the data into float"
      ],
      "metadata": {
        "id": "KEJOo5OV2HtY"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_padding_mask(seq):\n",
        "    #seq is the sentenece after padding and digitalizing\n",
        "\n",
        "    #tf.math.equal(x,y)\n",
        "    #tf.cast()change the type of data\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32) #(batch_size, seq_len)\n",
        "\n",
        "    #add new dimension into padding\n",
        "    #将add attention into logits\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :] #(batch_size, 1, 1, seq_len)"
      ],
      "metadata": {
        "id": "shxQVTHC28-q"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_look_ahead_mask(size):\n",
        "    #size is the length of the sentence\n",
        "\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size,size)), -1, 0)\n",
        "    return mask #(seq_len, seq_len)"
      ],
      "metadata": {
        "id": "u_S_8kes3Drq"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.ones((5,5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pa94Kc4IAsq0",
        "outputId": "ee36e060-c789-4305-ea55-f7c768fcc7e7"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5, 5), dtype=float32, numpy=\n",
              "array([[1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1.]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.linalg.band_part(tf.ones((5, 5)), -1, 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDhJ1y_IAvpX",
        "outputId": "30dff9b6-431f-4597-e073-ee382ae1db7f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5, 5), dtype=float32, numpy=\n",
              "array([[1., 0., 0., 0., 0.],\n",
              "       [1., 1., 0., 0., 0.],\n",
              "       [1., 1., 1., 0., 0.],\n",
              "       [1., 1., 1., 1., 0.],\n",
              "       [1., 1., 1., 1., 1.]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#when num_lower, num_upper are all 0, only store the value on the diagonal\n",
        "tf.linalg.band_part(tf.ones((5, 5)), 0, 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ml8SAz8gA0OF",
        "outputId": "f8a79a3c-d3de-47ee-d356-a1e261e935bf"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5, 5), dtype=float32, numpy=\n",
              "array([[1., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0.],\n",
              "       [0., 0., 1., 0., 0.],\n",
              "       [0., 0., 0., 1., 0.],\n",
              "       [0., 0., 0., 0., 1.]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#when num_lower is 1, only store the value under the diagonal\n",
        "tf.linalg.band_part(tf.ones((5, 5)), 1, 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddzgpY8eA12K",
        "outputId": "275c8817-5ef3-4147-dbf6-2ecc4a0fc466"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5, 5), dtype=float32, numpy=\n",
              "array([[1., 0., 0., 0., 0.],\n",
              "       [1., 1., 0., 0., 0.],\n",
              "       [0., 1., 1., 0., 0.],\n",
              "       [0., 0., 1., 1., 0.],\n",
              "       [0., 0., 0., 1., 1.]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#when num_upper is 1, only store the value above the diagonal\n",
        "tf.linalg.band_part(tf.ones((5, 5)), 0, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LKZEQO4A4mi",
        "outputId": "f1a31379-91af-4682-912a-3d21cc08ec30"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5, 5), dtype=float32, numpy=\n",
              "array([[1., 1., 0., 0., 0.],\n",
              "       [0., 1., 1., 0., 0.],\n",
              "       [0., 0., 1., 1., 0.],\n",
              "       [0., 0., 0., 1., 1.],\n",
              "       [0., 0., 0., 0., 1.]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sequence mask apply in the decoder\n",
        "\n",
        "x = tf.random.uniform((3, 5)) # we assume there are 3 sentence, and length is 5\n",
        "temp1 = create_look_ahead_mask(x.shape[1])\n",
        "temp1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTZzQhzZA9Pa",
        "outputId": "51a50b96-ddfa-4dc4-cac0-fdfbda8e7d2c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5, 5), dtype=float32, numpy=\n",
              "array([[0., 1., 1., 1., 1.],\n",
              "       [0., 0., 1., 1., 1.],\n",
              "       [0., 0., 0., 1., 1.],\n",
              "       [0., 0., 0., 0., 1.],\n",
              "       [0., 0., 0., 0., 0.]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#temp2 we create a padding_mask\n",
        "x = tf.constant([[1, 3, 2, 3, 0], [5, 2, 3, 0, 0], [7, 8, 0, 0, 0 ]])\n",
        "temp2 = create_padding_mask(x)\n",
        "temp2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SkP-HShBNe2",
        "outputId": "138446e1-686b-4f1c-bbde-fc34d9c5e710"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 1, 1, 5), dtype=float32, numpy=\n",
              "array([[[[0., 0., 0., 0., 1.]]],\n",
              "\n",
              "\n",
              "       [[[0., 0., 0., 1., 1.]]],\n",
              "\n",
              "\n",
              "       [[[0., 0., 1., 1., 1.]]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#combine the temp1 and temp2, and return the maximum of them\n",
        "sequence_mask = tf.maximum(temp2, temp1)\n",
        "sequence_mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuC3PfZbBPJJ",
        "outputId": "9179f96e-1b5d-4d21-9de3-240be512fb7b"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 1, 5, 5), dtype=float32, numpy=\n",
              "array([[[[0., 1., 1., 1., 1.],\n",
              "         [0., 0., 1., 1., 1.],\n",
              "         [0., 0., 0., 1., 1.],\n",
              "         [0., 0., 0., 0., 1.],\n",
              "         [0., 0., 0., 0., 1.]]],\n",
              "\n",
              "\n",
              "       [[[0., 1., 1., 1., 1.],\n",
              "         [0., 0., 1., 1., 1.],\n",
              "         [0., 0., 0., 1., 1.],\n",
              "         [0., 0., 0., 1., 1.],\n",
              "         [0., 0., 0., 1., 1.]]],\n",
              "\n",
              "\n",
              "       [[[0., 1., 1., 1., 1.],\n",
              "         [0., 0., 1., 1., 1.],\n",
              "         [0., 0., 1., 1., 1.],\n",
              "         [0., 0., 1., 1., 1.],\n",
              "         [0., 0., 1., 1., 1.]]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    \"\"\"计算注意力权重。\n",
        "    q, k, v 必须具有匹配的前置维度。\n",
        "    k, v 必须有匹配的倒数第二个维度，例如：seq_len_k = seq_len_v。\n",
        "    虽然 mask 根据其类型（填充或前瞻）有不同的形状，\n",
        "    但是 mask 必须能进行广播转换以便求和。\n",
        "\n",
        "    参数:\n",
        "    q: 请求的形状 == (..., seq_len_q, depth)\n",
        "    k: 主键的形状 == (..., seq_len_k, depth)\n",
        "    v: 数值的形状 == (..., seq_len_v, depth_v)\n",
        "    mask: Float 张量，其形状能转换成\n",
        "          (..., seq_len_q, seq_len_k)。默认为None。\n",
        "\n",
        "    返回值:\n",
        "    输出，注意力权重\n",
        "    \"\"\"\n",
        "\n",
        "    # define and calculate the attention weight\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True) # （batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "\n",
        "    # scale matmul_qk\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    # add the mask to the scaled tensor\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "    # add up to 1.\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1) # （batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "\n",
        "    #tf.matmul attention_weights times v\n",
        "    output = tf.matmul(attention_weights, v) # （batch_size, num_heads, seq_len_q, depth_v)\n",
        "\n",
        "    return output, attention_weights"
      ],
      "metadata": {
        "id": "ikouTvRTBSaW"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, d_model, num_heads): #we assume d_model=512, num_heads=8\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        #assertion function\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads # depth=64\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size): #we assume batch_size=32, x is q,k,v shape are all(batch_size, seq_len, d_model)\n",
        "        \"\"\"Split the last dimension into (num_heads, depth).\n",
        "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "        \"\"\"\n",
        "        '''分拆最后一个维度到(num_heads, depth)\n",
        "        转置结果使得形状为(batch_size, num_heads, seq_len, depth)\n",
        "        '''\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "\n",
        "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "            q, k, v, mask)\n",
        "\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "        concat_attention = tf.reshape(scaled_attention,\n",
        "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mEFMMkzJBXdn"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#d_model must be divided evenly by num_heads\n",
        "temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
        "y = tf.random.uniform((32, 60, 512))  # (batch_size, encoder_sequence, d_model)\n",
        "out, attn= temp_mha(y, k=y, q=y, mask=None)\n",
        "out.shape, attn.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjdgQYpfl3tT",
        "outputId": "432cdb31-003a-4eea-f81e-1aa513e73700"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([32, 60, 512]), TensorShape([32, 8, 60, 60]))"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# realize the above, and get the result Z\n",
        "# assume we have 2 heads\n",
        "x = tf.random.normal([2,4])\n",
        "\n",
        "wq0 = tf.random.normal([4,3])\n",
        "wk0 = tf.random.normal([4,3])\n",
        "wv0 = tf.random.normal([4,3])\n",
        "print(wq0)\n",
        "print(wk0)\n",
        "print(wv0)\n",
        "\n",
        "wq1 = tf.random.normal([4,3])\n",
        "wk1 = tf.random.normal([4,3])\n",
        "wv1 = tf.random.normal([4,3])\n",
        "print(wq1)\n",
        "print(wk1)\n",
        "print(wv1)\n",
        "\n",
        "q0 = tf.matmul(x, wq0)\n",
        "k0 = tf.matmul(x, wk0)\n",
        "v0 = tf.matmul(x, wv0)\n",
        "print(q0)\n",
        "print(k0)\n",
        "print(v0)\n",
        "\n",
        "q1 = tf.matmul(x, wq1)\n",
        "k1 = tf.matmul(x, wk1)\n",
        "v1 = tf.matmul(x, wv1)\n",
        "print(q1)\n",
        "print(k1)\n",
        "print(v1)\n",
        "\n",
        "#上面求出了q0,q1,k0,k1,v0,v1.我们进行self-attention计算\n",
        "score0 = tf.matmul(q0, k0, transpose_b=True)\n",
        "score1 = tf.matmul(q1, k1, transpose_b=True)\n",
        "divide0 = score0/tf.sqrt(2.)\n",
        "divide1 = score1/tf.sqrt(2.)\n",
        "softmax0 = tf.nn.softmax(divide0)\n",
        "softmax1 = tf.nn.softmax(divide1)\n",
        "z0 = tf.matmul(softmax0, v0)\n",
        "z1 = tf.matmul(softmax1, v1)\n",
        "concat_z0_z1 = tf.concat([z0, z1], axis=1)\n",
        "wo = tf.random.normal([6, 4])\n",
        "Z = tf.matmul(concat_z0_z1, wo)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0imVEuBKzWi1",
        "outputId": "556f83d4-51cf-4379-a65d-b8671f5104b4"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[ 0.46622962  0.2080252  -0.84371126]\n",
            " [ 0.80454445 -0.4703905  -0.61515003]\n",
            " [-0.41130337 -0.92221004  0.8592183 ]\n",
            " [ 0.34704027 -1.7107095  -0.03712396]], shape=(4, 3), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[ 0.9348219  -1.4569345  -1.4892461 ]\n",
            " [ 1.1499858  -0.2787443   0.0275764 ]\n",
            " [ 1.0697577  -0.50963944  1.132039  ]\n",
            " [ 1.9215623   0.31058407  0.33645138]], shape=(4, 3), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[-0.26877266 -0.49885225 -1.4473995 ]\n",
            " [ 0.19996443  0.70790386 -0.6599549 ]\n",
            " [ 0.5505534   0.5891746  -1.5186629 ]\n",
            " [-1.1657658  -0.5380835  -0.14188465]], shape=(4, 3), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[ 0.2693474  -1.104103    1.6133176 ]\n",
            " [ 1.5124226   1.4598664  -0.22685947]\n",
            " [-0.62625355 -1.4525222  -0.86046547]\n",
            " [ 0.09839997 -0.6266672   0.9287517 ]], shape=(4, 3), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[ 2.2354498   0.83556646  0.644459  ]\n",
            " [-1.2137841   0.08957743 -1.5414487 ]\n",
            " [ 0.5464072  -0.5082599   1.8049049 ]\n",
            " [-0.53800225  0.57333916 -0.5404756 ]], shape=(4, 3), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[-0.1966728  -0.86936694  0.45782444]\n",
            " [ 0.48766527  1.2479094   0.22730759]\n",
            " [ 0.586218   -0.02269316  1.0989455 ]\n",
            " [-1.5412089  -0.04496105  0.9177846 ]], shape=(4, 3), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[-2.2488596   2.3293617   1.8069435 ]\n",
            " [ 1.6552832  -3.202582   -0.96647424]], shape=(2, 3), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[-3.60243    -0.49982518  0.22986194]\n",
            " [ 4.901951   -0.6393702   0.3017473 ]], shape=(2, 3), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[ 1.4992573  -0.2694587  -0.02951412]\n",
            " [-1.2461629   0.2587118  -2.0911021 ]], shape=(2, 3), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[-3.500616  -3.377351  -1.199415 ]\n",
            " [ 2.1802883  0.3794328  1.2726063]], shape=(2, 3), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[ 3.7984073 -1.186414   5.225691 ]\n",
            " [-1.4892361  1.0591627 -2.151536 ]], shape=(2, 3), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[ 1.4611323  -2.4883015  -0.61611956]\n",
            " [-1.3175322   1.3824174   2.0646706 ]], shape=(2, 3), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    #d_model, the depth of the embedding\n",
        "    #dff the neural number we want to add\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(dff, activation='relu'), #(batch_size, seq_len, dff)\n",
        "        tf.keras.layers.Dense(d_model) #(batch_size, seq_len, d_model)\n",
        "    ])"
      ],
      "metadata": {
        "id": "Qi3bIJCg42wR"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    # d_model we set 512 here\n",
        "    # num_heads we set 8\n",
        "    # dff we set 2048\n",
        "    # rate we add dropout in this part to prevent the overfittion\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads) #multihead self attention\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff) #feed forward network\n",
        "\n",
        "        #层标准化\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        #dropout层\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training, mask):\n",
        "\n",
        "        #get multi head attention\n",
        "        attn_output, _ = self.mha(x, x, x, mask) #(batch_size, input_seq_len, d_model)\n",
        "        #增加dropout层\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        #add x into residual calculation\n",
        "        out1 = self.layernorm1(x + attn_output) #(batch_size, input_seq_len, d_model)\n",
        "\n",
        "        #feed forward network\n",
        "        ffn_output = self.ffn(out1) #(batch_size, input_seq_len, d_model)\n",
        "        #dropout\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        #add x into the residual calculation, and layer normalizaitio\n",
        "        out2 = self.layernorm2(out1 + ffn_output) #(batch_size, input_seq_len, d_model)\n",
        "\n",
        "        return out2\n"
      ],
      "metadata": {
        "id": "Iz5Y3KsC42qK"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 测试encoderlayer类\n",
        "# initialize the encoder\n",
        "sample_encoder_layer = EncoderLayer(512, 8, 2048)\n",
        "\n",
        "#call the call method\n",
        "sample_encoder_layer_output = sample_encoder_layer(\n",
        "    tf.random.uniform((64, 43, 512)), False, None)\n",
        "\n",
        "sample_encoder_layer_output.shape  # (batch_size, input_seq_len, d_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZK_Z0Bir42in",
        "outputId": "927f6fe0-c5b8-4c51-b753-3b134ac23bb0"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 43, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        # multi-head attention\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        #combine the encoder and decoder\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        #feed forward network\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        #layer normalizaition\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        #decoder attention, residual, and layernorm\n",
        "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask) #(batch_size, target_seq_len, d_model)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "        #(batch_size, target_seq_len, d_model)\n",
        "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        return out3, attn_weights_block1, attn_weights_block2\n"
      ],
      "metadata": {
        "id": "8T0wWVp1APZN"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_decoder_layer = DecoderLayer(512, 8, 2048)\n",
        "\n",
        "sample_decoder_layer_output, _, _ = sample_decoder_layer(\n",
        "    tf.random.uniform((64, 50, 512)), sample_encoder_layer_output,\n",
        "    False, None, None)\n",
        "\n",
        "sample_decoder_layer_output.shape  # (batch_size, target_seq_len, d_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3AWer6KAPWd",
        "outputId": "31227601-aa5b-44b2-a866-22c19be7c2e7"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 50, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "                 maximum_position_encoding, rate=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "        #num_layers, represent the number of encoderlayer\n",
        "        #d_model, represent the depth of embedding的\n",
        "        #num_heads\n",
        "        #dff\n",
        "        #input_vocab_size\n",
        "        #maximum_position_encoding\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        #embedding\n",
        "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "        #position embadding\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
        "\n",
        "        #initialize num_layers of EncoderLayer\n",
        "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training, mask):\n",
        "        seq_len = tf.shape(x)[1] #get the longest sentence\n",
        "\n",
        "        #add the information to every word\n",
        "        x = self.embedding(x) #(batch_size, input_seq_len, d_model)\n",
        "        #x times d_moder.sqrt is to enhance semantic information and weaken positional information\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        #loop EncoderLayer for num_layers times\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training, mask)\n",
        "\n",
        "        return x #(batch_size, input_seq_len, d_model)\n"
      ],
      "metadata": {
        "id": "mgFyyyr1APTw"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_encoder = Encoder(num_layers=2, d_model=512, num_heads=8,\n",
        "                         dff=2048, input_vocab_size=8500,\n",
        "                         maximum_position_encoding=10000)\n",
        "temp_input = tf.random.uniform((64, 62), dtype=tf.int64, minval=0, maxval=200)\n",
        "\n",
        "sample_encoder_output = sample_encoder(temp_input, training=False, mask=None)\n",
        "\n",
        "print(sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXTNEiQaAPRL",
        "outputId": "a8251943-1b1c-4fa4-9f95-b6d10ad27f11"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 62, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
        "                 maximum_position_encoding, rate=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        #embedding\n",
        "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "        #position embadding\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "        # initialize num_layers DecoderLayers\n",
        "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "\n",
        "        seq_len = tf.shape(x)[1] #seq_len is target_seq_len\n",
        "        attention_weights = {}\n",
        "\n",
        "        #same as encoder\n",
        "        x = self.embedding(x) #(batch_size, target_seq_len, d_model)\n",
        "\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n",
        "\n",
        "            attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
        "            attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
        "\n",
        "        #x.shape == (batch_size, target_seq_len, d_model)\n",
        "        return x, attention_weights"
      ],
      "metadata": {
        "id": "IgHlIuSmAdFt"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_decoder = Decoder(num_layers=2, d_model=512, num_heads=8,\n",
        "                         dff=2048, target_vocab_size=8000,\n",
        "                         maximum_position_encoding=5000)\n",
        "temp_input = tf.random.uniform((64, 26), dtype=tf.int64, minval=0, maxval=200)\n",
        "\n",
        "output, attn = sample_decoder(temp_input,\n",
        "                              enc_output=sample_encoder_output,\n",
        "                              training=False,\n",
        "                              look_ahead_mask=None,\n",
        "                              padding_mask=None)\n",
        "\n",
        "output.shape, attn['decoder_layer2_block2'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYN_rkLDAe-a",
        "outputId": "d9f2d331-0a12-4375-98f7-ade34d3dcb9e"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 26, 512]), TensorShape([64, 8, 26, 62]))"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    #define transformer with num_layers, d_model, num_heads, dff,input_vocab_size,target_vocab_size,pe_input,pe_target\n",
        "\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "                 target_vocab_size, pe_input, pe_target, rate=0.1):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        #transformer encoder\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n",
        "        #transformer decoder\n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
        "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        # Keras models prefer if you pass all your inputs in the first argument\n",
        "        inp, tar = inputs\n",
        "\n",
        "        #create masks\n",
        "        enc_padding_mask, look_ahead_mask, dec_padding_mask = self.create_masks(inp, tar)\n",
        "\n",
        "        enc_output = self.encoder(inp, training, enc_padding_mask) #(batch_size, inp_seq_len, d_model)\n",
        "\n",
        "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "        dec_output, attention_weights = self.decoder(\n",
        "            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "\n",
        "        final_output = self.final_layer(dec_output) #(batch_size, tar_seq_len, target_vocab_size)\n",
        "\n",
        "        return final_output, attention_weights\n",
        "\n",
        "    def create_masks(self, inp, tar):\n",
        "        #Encoder padding mask\n",
        "        enc_padding_mask = create_padding_mask(inp) #（batch_size, 1, 1, inp_seq_len)\n",
        "\n",
        "        #Used in the 2nd attention block in the decoder.\n",
        "        #This padding mask is used to mask the encoder outputs.\n",
        "        dec_padding_mask = create_padding_mask(inp) #（batch_size, 1, 1, inp_seq_len)\n",
        "\n",
        "        # Used in the 1st attention block in the decoder.\n",
        "        # It is used to pad and mask future tokens in the input received by\n",
        "        # the decoder.\n",
        "        look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1]) #(tar_seq_len, tar_seq_len)\n",
        "        dec_target_padding_mask = create_padding_mask(tar) #(batch_size, 1, 1, tar_seq_len)\n",
        "        #tf.maximum(x,y) return the maximum of  x and y\n",
        "        look_ahead_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask) #（batch_size, 1, tar_seq_len, tar_seq_len)\n",
        "\n",
        "        return enc_padding_mask, look_ahead_mask, dec_padding_mask\n"
      ],
      "metadata": {
        "id": "yqIcIbflAnIX"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_transformer = Transformer(\n",
        "    num_layers=2, d_model=512, num_heads=8, dff=2048,\n",
        "    input_vocab_size=8500, target_vocab_size=8000,\n",
        "    pe_input=10000, pe_target=6000)\n",
        "\n",
        "temp_input = tf.random.uniform((1, 38), dtype=tf.int64, minval=0, maxval=200)\n",
        "temp_target = tf.random.uniform((1, 4), dtype=tf.int64, minval=0, maxval=200)\n",
        "\n",
        "fn_out, _ = sample_transformer([temp_input, temp_target], training=False)\n",
        "\n",
        "fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yy5mgKzXApVT",
        "outputId": "e2546ae3-e17f-4363-8475-bb56eba49599"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([1, 4, 8000])"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_layers = 4\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "dropout_rate = 0.1"
      ],
      "metadata": {
        "id": "c_5E1unpCWpp"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        step = tf.cast(step, tf.float32)  # step is float32\n",
        "        #step is step_num, arg1 is the value continuously decrease\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        #arg2 is the value continusously increase\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        #arg1 > arg2 at the beginning, when warmup_steps(4000), arg1 == arg2, when 4001, arg1 < arg2\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "metadata": {
        "id": "fZJ8fUohCbzG"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp_learning_rate_scheldule = CustomSchedule(d_model)\n",
        "\n",
        "plt.plot(temp_learning_rate_scheldule(tf.range(40000, dtype=tf.float32)))\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "plt.xlabel(\"Train Step\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "YNRHsy-mCdO_",
        "outputId": "8d85228b-e1ed-4392-df44-48b30ddadb1e"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Train Step')"
            ]
          },
          "metadata": {},
          "execution_count": 53
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "#optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n"
      ],
      "metadata": {
        "id": "8X3RQXSLCfBi"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Sparse cross-entropy loss function\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')"
      ],
      "metadata": {
        "id": "AEYFKc_SCnY-"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_function(real, pred):\n",
        "    #real 真实值 shape （batch_size, target_seq_len)\n",
        "    #这里的target_seq_len 要去掉句子开头的start, 所以实际长度要减一\n",
        "    #pred 预测值 shape （batch_size, tar_seq_len, target_vocab_size)\n",
        "    #预测值里的tar_seq_len是没有start的，因为start是作为预测的输入，\n",
        "    #而输出是start后面的第一个单词\n",
        "\n",
        "    #tf.math.equal(real,0) 0 is true，not 0 is false\n",
        "    #tf.math.logical_not(tf.math.equal(real, 0)) change true into false， false into true\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "\n",
        "    #get the loss\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    #change the data type\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "\n",
        "    #delete mask values,\n",
        "    loss_ *= mask\n",
        "\n",
        "    #the average of loss\n",
        "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
      ],
      "metadata": {
        "id": "HEWS8zRdCpNx"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy_function(real, pred):\n",
        "    #real (batch_size, target_seq_len) target_seq_len == loss real\n",
        "    #pred (batch_size, tar_seq_len, target_vocab_size)\n",
        "    #tar_seq_len == target_seq_len\n",
        "\n",
        "    #tf.argmax 取pred里的最大值，axis=2 表示取第3个维度target_vocab_size里的最大值\n",
        "    #最后shape为（batch_size, tar_seq_len)\n",
        "    #tf.equal 就是拿real和刚取了最大值的pred进行比较\n",
        "    #值相等的单词就是true 否则为false\n",
        "    accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n",
        "\n",
        "    mask = tf.math.logical_not(tf.math.equal(real,0))\n",
        "\n",
        "    #if mask == accuracies: true else: false\n",
        "    accuracies = tf.math.logical_and(mask, accuracies)\n",
        "\n",
        "    accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
        "    mask = tf.cast(mask, dtype=tf.float32)\n",
        "\n",
        "    #tf.reduce_sum(accuracies) how many word that is predicted correct\n",
        "    #tf.reduce_sum(mask) how many word in total\n",
        "    return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)\n",
        "\n",
        "# loss and  accuracy\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')"
      ],
      "metadata": {
        "id": "1SZVyEcDCrD-"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text_processor.vocabulary_size()  #input size\n",
        "output_text_processor.vocabulary_size() #output size\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jq-uD8odCu6_",
        "outputId": "65ddaff6-409b-4a5e-f72d-9789a177ff8f"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7180"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize Transformer\n",
        "#num_layers, 表示decodrlayer部件要有几个\n",
        "#d_model, 表示embedding的深度是多少\n",
        "#num_heads 表示多头注意力里面的多头数是多少\n",
        "#dff 表示增加非线性时设置的神经元个数\n",
        "#input_vocab_size 输入的词典大小\n",
        "#target_vocab_size 输出的词典大小\n",
        "#这里的pe_input 就是前面的maximum_position_encoding 最大句子的长度 这里是输入句子的最大长度\n",
        "#这里的pe_target 就是前面的maximum_position_encoding 最大句子的长度 这里是输出句子的最大长度\n",
        "\n",
        "transformer = Transformer(\n",
        "    num_layers=num_layers,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    input_vocab_size=input_text_processor.vocabulary_size(),\n",
        "    target_vocab_size=output_text_processor.vocabulary_size(),\n",
        "    pe_input=1000,\n",
        "    pe_target=1000,\n",
        "    rate=dropout_rate)"
      ],
      "metadata": {
        "id": "WaSn5GSYC1G_"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = \"./checkpoints/cn_en/train\" #检测文件保存位置\n",
        "\n",
        "#the content stored in check point\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "#max_to_keep=5\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "#recover the newest check point\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print('Latest checkpoint restored!!')"
      ],
      "metadata": {
        "id": "5M_lbcz2C1EU"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 20\n"
      ],
      "metadata": {
        "id": "UV2Azw-JC1Bw"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define train_step_signature\n",
        "#列表里含有2个被tf.TensorSpec定义的张量\n",
        "train_step_signature = [\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "]\n",
        "train_step_signature"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtAxuClQC8n0",
        "outputId": "be8b2fa4-a36c-4fe7-cbfa-878dd74b70d2"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[TensorSpec(shape=(None, None), dtype=tf.int64, name=None),\n",
              " TensorSpec(shape=(None, None), dtype=tf.int64, name=None)]"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#input_signature的好处：\n",
        "#1.可以限定函数的输入类型，以防止调用函数时调错，\n",
        "#2．一个函数有了input_signature之后，在tensorflow里边才可以保存成savedmodel。\n",
        "#在保存成savedmodel的过程中，需要使用get_concrete_function函数把一个tf.function\n",
        "#标注的普通的python函数变成带有图定义的函数。\n",
        "\n",
        "# tf.function 模块\n",
        "#我们仅需加入一个简单的 @tf.function 修饰符，就能轻松将模型以图模式运行！\n",
        "\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def train_step(inp, tar):\n",
        "    tar_inp = tar[:, :-1]\n",
        "    tar_real = tar[:, 1:]\n",
        "\n",
        "    #tf.GradientTape() used to calculate the gradient\n",
        "    with tf.GradientTape() as tape:\n",
        "        #get predicted values\n",
        "        predictions, _ = transformer([inp, tar_inp], training=True)\n",
        "        #get loss\n",
        "        loss = loss_function(tar_real, predictions)\n",
        "    #calculate the gradient\n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "    #apply gradient\n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "    #calculate the loss and accuracy\n",
        "    train_loss(loss)\n",
        "    train_accuracy(accuracy_function(tar_real, predictions))"
      ],
      "metadata": {
        "id": "Zlq_7EaJC-1I"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(EPOCHS): #train the data for 20 epochs\n",
        "    start = time.time()\n",
        "\n",
        "    #initialize the loss and accuracy\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "\n",
        "    # inp -> China, tar -> english\n",
        "    for (batch, (inp, tar)) in enumerate(train_batches):\n",
        "        train_step(inp, tar)\n",
        "        #every 50 batch print one loss和accuray\n",
        "        if batch % 50 == 0:\n",
        "            print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "    #every 5个epoch store a check point\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        ckpt_save_path = ckpt_manager.save()\n",
        "        print(f'Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}')\n",
        "    #after every epoch, print the loss and accuracy\n",
        "    print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "    #print the training time of every epoch\n",
        "    print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gfpv8XcRDAxA",
        "outputId": "a170ee26-4167-4d16-cef9-ccf2131659cf"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 8.9026 Accuracy 0.0000\n",
            "Epoch 1 Batch 50 Loss 8.8054 Accuracy 0.0218\n",
            "Epoch 1 Batch 100 Loss 8.6600 Accuracy 0.0709\n",
            "Epoch 1 Batch 150 Loss 8.5137 Accuracy 0.0882\n",
            "Epoch 1 Batch 200 Loss 8.3441 Accuracy 0.0968\n",
            "Epoch 1 Batch 250 Loss 8.1402 Accuracy 0.1078\n",
            "Epoch 1 Batch 300 Loss 7.9108 Accuracy 0.1266\n",
            "Epoch 1 Batch 350 Loss 7.6733 Accuracy 0.1411\n",
            "Epoch 1 Batch 400 Loss 7.4394 Accuracy 0.1517\n",
            "Epoch 1 Batch 450 Loss 7.2218 Accuracy 0.1608\n",
            "Epoch 1 Loss 7.1513 Accuracy 0.1644\n",
            "Time taken for 1 epoch: 75.40 secs\n",
            "\n",
            "Epoch 2 Batch 0 Loss 5.2011 Accuracy 0.2777\n",
            "Epoch 2 Batch 50 Loss 5.1676 Accuracy 0.2739\n",
            "Epoch 2 Batch 100 Loss 5.0694 Accuracy 0.2788\n",
            "Epoch 2 Batch 150 Loss 4.9826 Accuracy 0.2850\n",
            "Epoch 2 Batch 200 Loss 4.9059 Accuracy 0.2916\n",
            "Epoch 2 Batch 250 Loss 4.8340 Accuracy 0.2991\n",
            "Epoch 2 Batch 300 Loss 4.7632 Accuracy 0.3060\n",
            "Epoch 2 Batch 350 Loss 4.7027 Accuracy 0.3110\n",
            "Epoch 2 Batch 400 Loss 4.6450 Accuracy 0.3162\n",
            "Epoch 2 Batch 450 Loss 4.5941 Accuracy 0.3209\n",
            "Epoch 2 Loss 4.5747 Accuracy 0.3226\n",
            "Time taken for 1 epoch: 30.73 secs\n",
            "\n",
            "Epoch 3 Batch 0 Loss 4.1554 Accuracy 0.3538\n",
            "Epoch 3 Batch 50 Loss 4.0158 Accuracy 0.3683\n",
            "Epoch 3 Batch 100 Loss 3.9679 Accuracy 0.3730\n",
            "Epoch 3 Batch 150 Loss 3.9271 Accuracy 0.3770\n",
            "Epoch 3 Batch 200 Loss 3.9040 Accuracy 0.3792\n",
            "Epoch 3 Batch 250 Loss 3.8760 Accuracy 0.3820\n",
            "Epoch 3 Batch 300 Loss 3.8557 Accuracy 0.3839\n",
            "Epoch 3 Batch 350 Loss 3.8341 Accuracy 0.3864\n",
            "Epoch 3 Batch 400 Loss 3.8106 Accuracy 0.3889\n",
            "Epoch 3 Batch 450 Loss 3.7872 Accuracy 0.3912\n",
            "Epoch 3 Loss 3.7801 Accuracy 0.3919\n",
            "Time taken for 1 epoch: 36.48 secs\n",
            "\n",
            "Epoch 4 Batch 0 Loss 3.4709 Accuracy 0.4237\n",
            "Epoch 4 Batch 50 Loss 3.4692 Accuracy 0.4165\n",
            "Epoch 4 Batch 100 Loss 3.4638 Accuracy 0.4191\n",
            "Epoch 4 Batch 150 Loss 3.4539 Accuracy 0.4202\n",
            "Epoch 4 Batch 200 Loss 3.4392 Accuracy 0.4214\n",
            "Epoch 4 Batch 250 Loss 3.4306 Accuracy 0.4221\n",
            "Epoch 4 Batch 300 Loss 3.4199 Accuracy 0.4229\n",
            "Epoch 4 Batch 350 Loss 3.4039 Accuracy 0.4253\n",
            "Epoch 4 Batch 400 Loss 3.3923 Accuracy 0.4271\n",
            "Epoch 4 Batch 450 Loss 3.3774 Accuracy 0.4290\n",
            "Epoch 4 Loss 3.3741 Accuracy 0.4294\n",
            "Time taken for 1 epoch: 29.10 secs\n",
            "\n",
            "Epoch 5 Batch 0 Loss 3.1458 Accuracy 0.4350\n",
            "Epoch 5 Batch 50 Loss 3.1074 Accuracy 0.4516\n",
            "Epoch 5 Batch 100 Loss 3.1036 Accuracy 0.4532\n",
            "Epoch 5 Batch 150 Loss 3.1032 Accuracy 0.4537\n",
            "Epoch 5 Batch 200 Loss 3.0925 Accuracy 0.4549\n",
            "Epoch 5 Batch 250 Loss 3.0847 Accuracy 0.4561\n",
            "Epoch 5 Batch 300 Loss 3.0759 Accuracy 0.4573\n",
            "Epoch 5 Batch 350 Loss 3.0699 Accuracy 0.4582\n",
            "Epoch 5 Batch 400 Loss 3.0607 Accuracy 0.4594\n",
            "Epoch 5 Batch 450 Loss 3.0536 Accuracy 0.4605\n",
            "Saving checkpoint for epoch 5 at ./checkpoints/cn_en/train/ckpt-1\n",
            "Epoch 5 Loss 3.0517 Accuracy 0.4610\n",
            "Time taken for 1 epoch: 27.33 secs\n",
            "\n",
            "Epoch 6 Batch 0 Loss 2.8389 Accuracy 0.4878\n",
            "Epoch 6 Batch 50 Loss 2.8272 Accuracy 0.4838\n",
            "Epoch 6 Batch 100 Loss 2.7979 Accuracy 0.4863\n",
            "Epoch 6 Batch 150 Loss 2.7849 Accuracy 0.4878\n",
            "Epoch 6 Batch 200 Loss 2.7813 Accuracy 0.4887\n",
            "Epoch 6 Batch 250 Loss 2.7766 Accuracy 0.4904\n",
            "Epoch 6 Batch 300 Loss 2.7742 Accuracy 0.4916\n",
            "Epoch 6 Batch 350 Loss 2.7600 Accuracy 0.4939\n",
            "Epoch 6 Batch 400 Loss 2.7509 Accuracy 0.4957\n",
            "Epoch 6 Batch 450 Loss 2.7416 Accuracy 0.4980\n",
            "Epoch 6 Loss 2.7393 Accuracy 0.4985\n",
            "Time taken for 1 epoch: 31.86 secs\n",
            "\n",
            "Epoch 7 Batch 0 Loss 2.5019 Accuracy 0.5393\n",
            "Epoch 7 Batch 50 Loss 2.4157 Accuracy 0.5401\n",
            "Epoch 7 Batch 100 Loss 2.4330 Accuracy 0.5369\n",
            "Epoch 7 Batch 150 Loss 2.4413 Accuracy 0.5359\n",
            "Epoch 7 Batch 200 Loss 2.4380 Accuracy 0.5364\n",
            "Epoch 7 Batch 250 Loss 2.4434 Accuracy 0.5363\n",
            "Epoch 7 Batch 300 Loss 2.4387 Accuracy 0.5380\n",
            "Epoch 7 Batch 350 Loss 2.4418 Accuracy 0.5386\n",
            "Epoch 7 Batch 400 Loss 2.4339 Accuracy 0.5399\n",
            "Epoch 7 Batch 450 Loss 2.4283 Accuracy 0.5410\n",
            "Epoch 7 Loss 2.4255 Accuracy 0.5417\n",
            "Time taken for 1 epoch: 27.31 secs\n",
            "\n",
            "Epoch 8 Batch 0 Loss 2.1532 Accuracy 0.5588\n",
            "Epoch 8 Batch 50 Loss 2.1029 Accuracy 0.5823\n",
            "Epoch 8 Batch 100 Loss 2.1128 Accuracy 0.5823\n",
            "Epoch 8 Batch 150 Loss 2.1378 Accuracy 0.5788\n",
            "Epoch 8 Batch 200 Loss 2.1361 Accuracy 0.5790\n",
            "Epoch 8 Batch 250 Loss 2.1430 Accuracy 0.5789\n",
            "Epoch 8 Batch 300 Loss 2.1478 Accuracy 0.5791\n",
            "Epoch 8 Batch 350 Loss 2.1497 Accuracy 0.5802\n",
            "Epoch 8 Batch 400 Loss 2.1554 Accuracy 0.5798\n",
            "Epoch 8 Batch 450 Loss 2.1574 Accuracy 0.5803\n",
            "Epoch 8 Loss 2.1586 Accuracy 0.5806\n",
            "Time taken for 1 epoch: 26.33 secs\n",
            "\n",
            "Epoch 9 Batch 0 Loss 1.8959 Accuracy 0.6031\n",
            "Epoch 9 Batch 50 Loss 1.8465 Accuracy 0.6210\n",
            "Epoch 9 Batch 100 Loss 1.8785 Accuracy 0.6169\n",
            "Epoch 9 Batch 150 Loss 1.8916 Accuracy 0.6155\n",
            "Epoch 9 Batch 200 Loss 1.9076 Accuracy 0.6133\n",
            "Epoch 9 Batch 250 Loss 1.9130 Accuracy 0.6129\n",
            "Epoch 9 Batch 300 Loss 1.9115 Accuracy 0.6137\n",
            "Epoch 9 Batch 350 Loss 1.9207 Accuracy 0.6131\n",
            "Epoch 9 Batch 400 Loss 1.9242 Accuracy 0.6130\n",
            "Epoch 9 Batch 450 Loss 1.9288 Accuracy 0.6128\n",
            "Epoch 9 Loss 1.9291 Accuracy 0.6130\n",
            "Time taken for 1 epoch: 26.16 secs\n",
            "\n",
            "Epoch 10 Batch 0 Loss 1.6922 Accuracy 0.6523\n",
            "Epoch 10 Batch 50 Loss 1.6455 Accuracy 0.6558\n",
            "Epoch 10 Batch 100 Loss 1.6630 Accuracy 0.6510\n",
            "Epoch 10 Batch 150 Loss 1.6696 Accuracy 0.6500\n",
            "Epoch 10 Batch 200 Loss 1.6757 Accuracy 0.6495\n",
            "Epoch 10 Batch 250 Loss 1.6826 Accuracy 0.6488\n",
            "Epoch 10 Batch 300 Loss 1.6918 Accuracy 0.6480\n",
            "Epoch 10 Batch 350 Loss 1.6962 Accuracy 0.6479\n",
            "Epoch 10 Batch 400 Loss 1.7061 Accuracy 0.6472\n",
            "Epoch 10 Batch 450 Loss 1.7099 Accuracy 0.6471\n",
            "Saving checkpoint for epoch 10 at ./checkpoints/cn_en/train/ckpt-2\n",
            "Epoch 10 Loss 1.7112 Accuracy 0.6472\n",
            "Time taken for 1 epoch: 27.44 secs\n",
            "\n",
            "Epoch 11 Batch 0 Loss 1.4030 Accuracy 0.6862\n",
            "Epoch 11 Batch 50 Loss 1.3932 Accuracy 0.6945\n",
            "Epoch 11 Batch 100 Loss 1.4308 Accuracy 0.6881\n",
            "Epoch 11 Batch 150 Loss 1.4597 Accuracy 0.6844\n",
            "Epoch 11 Batch 200 Loss 1.4788 Accuracy 0.6819\n",
            "Epoch 11 Batch 250 Loss 1.4883 Accuracy 0.6807\n",
            "Epoch 11 Batch 300 Loss 1.5039 Accuracy 0.6789\n",
            "Epoch 11 Batch 350 Loss 1.5099 Accuracy 0.6784\n",
            "Epoch 11 Batch 400 Loss 1.5136 Accuracy 0.6787\n",
            "Epoch 11 Batch 450 Loss 1.5188 Accuracy 0.6781\n",
            "Epoch 11 Loss 1.5228 Accuracy 0.6778\n",
            "Time taken for 1 epoch: 29.06 secs\n",
            "\n",
            "Epoch 12 Batch 0 Loss 1.3179 Accuracy 0.7116\n",
            "Epoch 12 Batch 50 Loss 1.2622 Accuracy 0.7182\n",
            "Epoch 12 Batch 100 Loss 1.2861 Accuracy 0.7137\n",
            "Epoch 12 Batch 150 Loss 1.2972 Accuracy 0.7122\n",
            "Epoch 12 Batch 200 Loss 1.3098 Accuracy 0.7105\n",
            "Epoch 12 Batch 250 Loss 1.3238 Accuracy 0.7096\n",
            "Epoch 12 Batch 300 Loss 1.3348 Accuracy 0.7081\n",
            "Epoch 12 Batch 350 Loss 1.3452 Accuracy 0.7067\n",
            "Epoch 12 Batch 400 Loss 1.3558 Accuracy 0.7053\n",
            "Epoch 12 Batch 450 Loss 1.3618 Accuracy 0.7050\n",
            "Epoch 12 Loss 1.3655 Accuracy 0.7046\n",
            "Time taken for 1 epoch: 26.11 secs\n",
            "\n",
            "Epoch 13 Batch 0 Loss 1.1756 Accuracy 0.7267\n",
            "Epoch 13 Batch 50 Loss 1.1669 Accuracy 0.7348\n",
            "Epoch 13 Batch 100 Loss 1.1526 Accuracy 0.7377\n",
            "Epoch 13 Batch 150 Loss 1.1711 Accuracy 0.7345\n",
            "Epoch 13 Batch 200 Loss 1.1850 Accuracy 0.7323\n",
            "Epoch 13 Batch 250 Loss 1.1938 Accuracy 0.7310\n",
            "Epoch 13 Batch 300 Loss 1.2102 Accuracy 0.7280\n",
            "Epoch 13 Batch 350 Loss 1.2235 Accuracy 0.7266\n",
            "Epoch 13 Batch 400 Loss 1.2314 Accuracy 0.7259\n",
            "Epoch 13 Batch 450 Loss 1.2408 Accuracy 0.7247\n",
            "Epoch 13 Loss 1.2432 Accuracy 0.7246\n",
            "Time taken for 1 epoch: 25.73 secs\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.9827 Accuracy 0.7616\n",
            "Epoch 14 Batch 50 Loss 1.0057 Accuracy 0.7632\n",
            "Epoch 14 Batch 100 Loss 1.0278 Accuracy 0.7600\n",
            "Epoch 14 Batch 150 Loss 1.0500 Accuracy 0.7564\n",
            "Epoch 14 Batch 200 Loss 1.0676 Accuracy 0.7534\n",
            "Epoch 14 Batch 250 Loss 1.0754 Accuracy 0.7523\n",
            "Epoch 14 Batch 300 Loss 1.0899 Accuracy 0.7500\n",
            "Epoch 14 Batch 350 Loss 1.1042 Accuracy 0.7479\n",
            "Epoch 14 Batch 400 Loss 1.1166 Accuracy 0.7457\n",
            "Epoch 14 Batch 450 Loss 1.1281 Accuracy 0.7445\n",
            "Epoch 14 Loss 1.1318 Accuracy 0.7442\n",
            "Time taken for 1 epoch: 26.16 secs\n",
            "\n",
            "Epoch 15 Batch 0 Loss 1.1424 Accuracy 0.7411\n",
            "Epoch 15 Batch 50 Loss 0.9405 Accuracy 0.7745\n",
            "Epoch 15 Batch 100 Loss 0.9498 Accuracy 0.7738\n",
            "Epoch 15 Batch 150 Loss 0.9641 Accuracy 0.7718\n",
            "Epoch 15 Batch 200 Loss 0.9812 Accuracy 0.7685\n",
            "Epoch 15 Batch 250 Loss 0.9916 Accuracy 0.7670\n",
            "Epoch 15 Batch 300 Loss 1.0028 Accuracy 0.7653\n",
            "Epoch 15 Batch 350 Loss 1.0145 Accuracy 0.7637\n",
            "Epoch 15 Batch 400 Loss 1.0273 Accuracy 0.7616\n",
            "Epoch 15 Batch 450 Loss 1.0341 Accuracy 0.7609\n",
            "Saving checkpoint for epoch 15 at ./checkpoints/cn_en/train/ckpt-3\n",
            "Epoch 15 Loss 1.0384 Accuracy 0.7603\n",
            "Time taken for 1 epoch: 27.00 secs\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.9088 Accuracy 0.8193\n",
            "Epoch 16 Batch 50 Loss 0.8510 Accuracy 0.7933\n",
            "Epoch 16 Batch 100 Loss 0.8625 Accuracy 0.7903\n",
            "Epoch 16 Batch 150 Loss 0.8755 Accuracy 0.7886\n",
            "Epoch 16 Batch 200 Loss 0.8892 Accuracy 0.7858\n",
            "Epoch 16 Batch 250 Loss 0.9004 Accuracy 0.7843\n",
            "Epoch 16 Batch 300 Loss 0.9130 Accuracy 0.7820\n",
            "Epoch 16 Batch 350 Loss 0.9266 Accuracy 0.7798\n",
            "Epoch 16 Batch 400 Loss 0.9397 Accuracy 0.7777\n",
            "Epoch 16 Batch 450 Loss 0.9483 Accuracy 0.7769\n",
            "Epoch 16 Loss 0.9520 Accuracy 0.7763\n",
            "Time taken for 1 epoch: 26.57 secs\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.7192 Accuracy 0.8337\n",
            "Epoch 17 Batch 50 Loss 0.7857 Accuracy 0.8074\n",
            "Epoch 17 Batch 100 Loss 0.7824 Accuracy 0.8077\n",
            "Epoch 17 Batch 150 Loss 0.8003 Accuracy 0.8044\n",
            "Epoch 17 Batch 200 Loss 0.8145 Accuracy 0.8011\n",
            "Epoch 17 Batch 250 Loss 0.8311 Accuracy 0.7979\n",
            "Epoch 17 Batch 300 Loss 0.8466 Accuracy 0.7951\n",
            "Epoch 17 Batch 350 Loss 0.8554 Accuracy 0.7937\n",
            "Epoch 17 Batch 400 Loss 0.8645 Accuracy 0.7926\n",
            "Epoch 17 Batch 450 Loss 0.8751 Accuracy 0.7910\n",
            "Epoch 17 Loss 0.8787 Accuracy 0.7905\n",
            "Time taken for 1 epoch: 27.60 secs\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.6166 Accuracy 0.8343\n",
            "Epoch 18 Batch 50 Loss 0.7300 Accuracy 0.8184\n",
            "Epoch 18 Batch 100 Loss 0.7390 Accuracy 0.8155\n",
            "Epoch 18 Batch 150 Loss 0.7579 Accuracy 0.8119\n",
            "Epoch 18 Batch 200 Loss 0.7665 Accuracy 0.8100\n",
            "Epoch 18 Batch 250 Loss 0.7777 Accuracy 0.8074\n",
            "Epoch 18 Batch 300 Loss 0.7870 Accuracy 0.8061\n",
            "Epoch 18 Batch 350 Loss 0.7930 Accuracy 0.8055\n",
            "Epoch 18 Batch 400 Loss 0.8037 Accuracy 0.8042\n",
            "Epoch 18 Batch 450 Loss 0.8130 Accuracy 0.8027\n",
            "Epoch 18 Loss 0.8166 Accuracy 0.8023\n",
            "Time taken for 1 epoch: 26.31 secs\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.6444 Accuracy 0.8284\n",
            "Epoch 19 Batch 50 Loss 0.6643 Accuracy 0.8314\n",
            "Epoch 19 Batch 100 Loss 0.6782 Accuracy 0.8273\n",
            "Epoch 19 Batch 150 Loss 0.6857 Accuracy 0.8253\n",
            "Epoch 19 Batch 200 Loss 0.7013 Accuracy 0.8227\n",
            "Epoch 19 Batch 250 Loss 0.7131 Accuracy 0.8204\n",
            "Epoch 19 Batch 300 Loss 0.7231 Accuracy 0.8187\n",
            "Epoch 19 Batch 350 Loss 0.7367 Accuracy 0.8164\n",
            "Epoch 19 Batch 400 Loss 0.7467 Accuracy 0.8147\n",
            "Epoch 19 Batch 450 Loss 0.7569 Accuracy 0.8133\n",
            "Epoch 19 Loss 0.7590 Accuracy 0.8130\n",
            "Time taken for 1 epoch: 25.99 secs\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.4767 Accuracy 0.8859\n",
            "Epoch 20 Batch 50 Loss 0.6093 Accuracy 0.8428\n",
            "Epoch 20 Batch 100 Loss 0.6261 Accuracy 0.8385\n",
            "Epoch 20 Batch 150 Loss 0.6389 Accuracy 0.8362\n",
            "Epoch 20 Batch 200 Loss 0.6526 Accuracy 0.8326\n",
            "Epoch 20 Batch 250 Loss 0.6603 Accuracy 0.8314\n",
            "Epoch 20 Batch 300 Loss 0.6741 Accuracy 0.8283\n",
            "Epoch 20 Batch 350 Loss 0.6866 Accuracy 0.8263\n",
            "Epoch 20 Batch 400 Loss 0.6958 Accuracy 0.8249\n",
            "Epoch 20 Batch 450 Loss 0.7068 Accuracy 0.8230\n",
            "Saving checkpoint for epoch 20 at ./checkpoints/cn_en/train/ckpt-4\n",
            "Epoch 20 Loss 0.7095 Accuracy 0.8224\n",
            "Time taken for 1 epoch: 26.50 secs\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9tVAR9hKHDs",
        "outputId": "ece40a46-cdc4-43b3-f186-cec06e08046f"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Translator(tf.Module):\n",
        "    def __init__(self, input_text_processor, output_text_processor, transformer):\n",
        "        self.tokenizers_pt = input_text_processor\n",
        "        self.tokenizers_en = output_text_processor\n",
        "\n",
        "        self.output_token_string_from_index = (\n",
        "        tf.keras.layers.StringLookup(\n",
        "            vocabulary=output_text_processor.get_vocabulary(),\n",
        "            mask_token='',\n",
        "            invert=True))\n",
        "        self.transformer = transformer\n",
        "\n",
        "    def __call__(self, sentence, max_length=20):\n",
        "\n",
        "        assert isinstance(sentence, tf.Tensor)\n",
        "\n",
        "        if len(sentence.shape) == 0:\n",
        "            sentence = sentence[tf.newaxis]\n",
        "\n",
        "        sentence = self.tokenizers_pt(sentence)\n",
        "\n",
        "        encoder_input = sentence\n",
        "\n",
        "        start_and_end = self.tokenizers_en([''])[0]\n",
        "\n",
        "        start = start_and_end[0][tf.newaxis]\n",
        "\n",
        "        end = start_and_end[1][tf.newaxis]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #if dynamic_size set to True, then the array we grow automatically\n",
        "        output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
        "        #write(index, value) ：write the value into  index;\n",
        "        output_array = output_array.write(0, start)\n",
        "\n",
        "\n",
        "        for i in tf.range(max_length):\n",
        "            #output_array.stack()return all the values in output_array\n",
        "            #tf.transpose is the shape of tensor\n",
        "            #shape is （2，1）tf.transpose change into（1，2）\n",
        "            output = tf.transpose(output_array.stack())\n",
        "\n",
        "            #output value will create a predict value (batch_size, tar_seq_len, target_vocab_size)\n",
        "            #we input a sentence，batch_size = 1\n",
        "            #tar_seq_len will change with the change of output second dimentsion, 1,2,3\n",
        "            #target_vocab_size will not change\n",
        "            #（1, 1, 8000), (1, 2, 8000), (1, 3, 8000)\n",
        "            predictions, _ = self.transformer([encoder_input, output], training=False)\n",
        "\n",
        "            #change the last token from seq_len dimension\n",
        "            predictions = predictions[:, -1:, :]\n",
        "\n",
        "            #get the argmax\n",
        "            predicted_id = tf.argmax(predictions, axis=-1)\n",
        "\n",
        "            #append it into output_array\n",
        "            output_array = output_array.write(i+1, predicted_id[0])\n",
        "\n",
        "            #if id equals to end, means the prediction is over, break the loop\n",
        "            if predicted_id == end:\n",
        "                break\n",
        "\n",
        "        #tf.transpose change output_array into （1，tokens）\n",
        "        output = tf.transpose(output_array.stack())\n",
        "\n",
        "\n",
        "        #extract all the predicted words\n",
        "        #result_text_tokens = output_vocab[output[0].numpy()]\n",
        "        result_text_tokens = self.output_token_string_from_index(output)\n",
        "\n",
        "        #delete the start and end\n",
        "        #text_tokens = result_text_tokens[1:-1]\n",
        "        text_tokens = result_text_tokens[:, 1:-1]\n",
        "\n",
        "        #join the words into sentence\n",
        "        #text = ' '.join(text_tokens)\n",
        "        text = tf.strings.reduce_join(text_tokens, axis=1, separator=' ')\n",
        "\n",
        "\n",
        "        _, attention_weights = self.transformer([encoder_input, output[:,:-1]], training=False)\n",
        "\n",
        "        return text[0], result_text_tokens, attention_weights"
      ],
      "metadata": {
        "id": "OFR-IL0BDICK"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#打印结果\n",
        "def print_translation(sentence, tokens, ground_truth):\n",
        "    print(f'{\"Input:\":15s}: {sentence}') #input sentence\n",
        "    print(f'{\"Prediction\":15s}: {tokens.numpy().decode(\"utf-8\")}') #predict value\n",
        "    print(f'{\"Ground truth\":15s}: {ground_truth}') #ground truth"
      ],
      "metadata": {
        "id": "8eF-nYXUDLmZ"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translator = Translator(input_text_processor, output_text_processor, transformer)\n"
      ],
      "metadata": {
        "id": "DLZG_JPeDPBm"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"想拿到大公司的面试可不容易\"\n",
        "ground_truth = \"Do you want to go out this weekend?\""
      ],
      "metadata": {
        "id": "DiDeS71DDQoS"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translated_text, translated_tokens, attention_weights = translator(\n",
        "    tf.constant(proce_cn_sentence(sentence)))\n",
        "print_translation(sentence, translated_text, ground_truth)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUgu_HUEDSKZ",
        "outputId": "cf58786f-4e43-42c4-a5ee-f49a1c6a4176"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:         : 想拿到大公司的面试可不容易\n",
            "Prediction     : it is not necessary for downtown .\n",
            "Ground truth   : Do you want to go out this weekend?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FOOUMhzvbAJy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}